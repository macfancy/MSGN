{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2ad9f-0e15-486a-88b3-b3ab8601f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import gamma, digamma, polygamma\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KernelDensity, NearestNeighbors\n",
    "from scipy.signal import find_peaks\n",
    "import networkx as nx\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def safe_log(x, epsilon=1e-10):\n",
    "    return np.log(np.maximum(x, epsilon))\n",
    "\n",
    "def h(x, mu, sigma, r, b, epsilon=1e-10):\n",
    "    b_safe = np.maximum(b, epsilon)\n",
    "    sigma_safe = np.maximum(sigma, epsilon)\n",
    "\n",
    "    term1 = b_safe / (gamma(1 / b_safe) * sigma_safe * 2 ** (1 + 1 / b_safe))\n",
    "\n",
    "    sign_term = 1 + r * np.sign(x - mu)\n",
    "    sign_term = np.maximum(sign_term, epsilon)\n",
    "\n",
    "    denominator = 2 * (sigma_safe ** b_safe) * sign_term ** b_safe\n",
    "    abs_diff = np.abs(x - mu) ** b_safe\n",
    "    term2 = np.exp(- abs_diff / np.maximum(denominator, epsilon))\n",
    "    return term1 * term2\n",
    "\n",
    "def weight(x, mu, sigma, r, b):\n",
    "    epsilon = 1e-8\n",
    "    max_value = 1e4\n",
    "\n",
    "    base_abs = np.maximum(np.abs(x - mu), epsilon)\n",
    "    base_sgn = np.maximum(np.abs(1 + r * np.sign(x - mu)), epsilon)\n",
    "\n",
    "    base_abs = np.minimum(base_abs, max_value)\n",
    "    base_sgn = np.minimum(base_sgn, max_value)\n",
    "\n",
    "    weight_value = (base_abs ** (b - 2) + epsilon) / (base_sgn ** b + epsilon)\n",
    "    weight_value = np.minimum(weight_value, max_value)\n",
    "    return weight_value\n",
    "\n",
    "def update_pi(q):\n",
    "    return np.mean(q)\n",
    "\n",
    "def update_mu(data, q, mu, sigma, r, b):\n",
    "    weights = np.zeros(len(q))\n",
    "    result = 0.0\n",
    "    result2 = 0.0\n",
    "    for i in range(len(q)):\n",
    "        weights[i] = weight(data[i], mu, sigma, r, b)\n",
    "        result += q[i] * weights[i]\n",
    "        result2 += q[i] * weights[i] * data[i]\n",
    "    return result2 / np.maximum(result, 1e-12)\n",
    "\n",
    "def update_sigma(data, pi, q, mu, sigma, r, b):\n",
    "    weights = np.zeros(len(q))\n",
    "    result = 0.0\n",
    "    for i in range(len(q)):\n",
    "        weights[i] = weight(data[i], mu, sigma, r, b)\n",
    "        result += q[i] * weights[i] * ((data[i] - mu) ** 2)\n",
    "\n",
    "    denominator = 2 * len(q) * np.maximum(pi, 1e-12)\n",
    "    return (b * result / np.maximum(denominator, 1e-12)) ** (1 / b)\n",
    "\n",
    "def update_r(data, q, mu, r, b):\n",
    "    epsilon = 1e-8\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    for i in range(len(q)):\n",
    "        pos_part = (data[i] >= mu).astype(float) * ((np.maximum(data[i] - mu, 0)) ** b)\n",
    "        neg_part = (data[i] < mu).astype(float) * ((np.maximum(mu - data[i], 0)) ** b)\n",
    "        numerator += q[i] * pos_part\n",
    "        denominator += q[i] * neg_part\n",
    "\n",
    "    if denominator < epsilon:\n",
    "        return 0.0\n",
    "\n",
    "    log_numerator = np.log(numerator + epsilon)\n",
    "    log_denominator = np.log(denominator + epsilon)\n",
    "    ratio = np.exp((log_numerator - log_denominator) / (b + 1))\n",
    "\n",
    "    r_new = 1 - 2 / (ratio + 1)\n",
    "    r_new = np.clip(r_new, -1, 1)\n",
    "    return r_new\n",
    "\n",
    "def f_val(x, mu, sigma, r, b, epsilon=1e-8):\n",
    "    return np.abs(x - mu) / (sigma * (1 + r * np.sign(x - mu)) + epsilon)\n",
    "\n",
    "def update_b(data, pi, q, mu, sigma, r, b, max_iter=10):\n",
    "    term2 = 0.0\n",
    "    term4 = 0.0\n",
    "    for i in range(len(q)):\n",
    "        fv = f_val(data[i], mu, sigma, r, b)\n",
    "        log_fv = safe_log(fv)\n",
    "        fv_clipped = np.clip(fv, 1e-5, 1e5)\n",
    "        log_fv_clipped = np.clip(log_fv, -1e5, 1e5)\n",
    "\n",
    "        term2 += q[i] * (fv_clipped ** b) * log_fv_clipped\n",
    "        term4 += q[i] * (fv_clipped ** b) * (log_fv_clipped ** 2)\n",
    "\n",
    "    term1 = (1 / b + np.log(2) / (b ** 2) + digamma(1 / b) / (b ** 2))\n",
    "    G = len(data) * np.maximum(pi, 1e-12) * term1 - 0.5 * term2\n",
    "\n",
    "    term3 = (1 / (b ** 2) + 2 * np.log(2) / (b ** 3) + 2 * digamma(1 / b) / (b ** 3) + polygamma(1, 1 / b) / (b ** 4))\n",
    "    H = -len(data) * np.maximum(pi, 1e-12) * term3 - 0.5 * term4\n",
    "\n",
    "    if np.abs(H) < 1e-12:\n",
    "        return b\n",
    "    b_new = b - G / H\n",
    "    if not np.isfinite(b_new) or b_new <= 1e-6:\n",
    "        b_new = max(b, 1e-3)\n",
    "    return b_new\n",
    "\n",
    "def log_likelihood(data, q, pi, mu, sigma, r, b, epsilon=1e-10, max_value=1e5, max_exp=500):\n",
    "    n, K = q.shape\n",
    "    likelihood = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(K):\n",
    "            ln_bj = safe_log(b[j], epsilon)\n",
    "            ln_term = (1 + 1 / b[j]) * np.log(2)\n",
    "            gamma_term = safe_log(gamma(1 / b[j]), epsilon)\n",
    "            sigma_term = safe_log(sigma[j], epsilon)\n",
    "            q_term = safe_log(np.maximum(q[i, j], epsilon), epsilon)\n",
    "\n",
    "            abs_diff = (np.abs(data[i] - mu[j])) ** b[j]\n",
    "            abs_diff = np.minimum(abs_diff, max_value)\n",
    "\n",
    "            sign_term = (1 + r[j] * np.sign(data[i] - mu[j]))\n",
    "            sign_term = np.maximum(sign_term, epsilon)\n",
    "\n",
    "            denominator = 2 * (sigma[j] ** b[j]) * (sign_term ** b[j])\n",
    "            denominator = np.minimum(denominator, max_value)\n",
    "\n",
    "            term2 = np.exp(-np.minimum(abs_diff / np.maximum(denominator, epsilon), max_exp))\n",
    "\n",
    "            likelihood += q[i, j] * (\n",
    "                ln_bj - ln_term - gamma_term - sigma_term - q_term - term2\n",
    "            )\n",
    "    return likelihood\n",
    "\n",
    "def em_algorithm(data, initial_params, max_iter=300, tol=1e-9):\n",
    "    pi, mu, sigma, r, b = initial_params\n",
    "    K = len(pi)\n",
    "    n = len(data)\n",
    "    q = np.zeros((n, K))\n",
    "    ll_prev = -np.inf\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        for j in range(K):\n",
    "            q[:, j] = pi[j] * h(data, mu[j], sigma[j], r[j], b[j])\n",
    "        q_sum = np.maximum(q.sum(axis=1, keepdims=True), 1e-12)\n",
    "        q = q / q_sum\n",
    "\n",
    "        pi_new = np.zeros(K)\n",
    "        mu_new = np.zeros(K)\n",
    "        sigma_new = np.zeros(K)\n",
    "        r_new = np.zeros(K)\n",
    "        b_new = np.zeros(K)\n",
    "\n",
    "        for j in range(K):\n",
    "            if K == 1:\n",
    "                pi_new[j] = 1.0\n",
    "            else:\n",
    "                pi_new[j] = update_pi(q[:, j])\n",
    "\n",
    "            mu_new[j] = update_mu(data, q[:, j], mu[j], sigma[j], r[j], b[j])\n",
    "            sigma_new[j] = update_sigma(data, pi_new[j], q[:, j], mu_new[j], sigma[j], r[j], b[j])\n",
    "            r_new[j] = update_r(data, q[:, j], mu_new[j], r[j], b[j])\n",
    "            b_new[j] = update_b(data, pi_new[j], q[:, j], mu_new[j], sigma_new[j], r_new[j], b[j])\n",
    "\n",
    "        pi, mu, sigma, r, b = pi_new, mu_new, sigma_new, r_new, b_new\n",
    "        ll_curr = log_likelihood(data, q, pi, mu, sigma, r, b)\n",
    "\n",
    "        if t > 0 and np.isfinite(ll_prev) and ll_prev != 0:\n",
    "            if np.abs(ll_curr / ll_prev - 1) < tol:\n",
    "                break\n",
    "        ll_prev = ll_curr\n",
    "\n",
    "    return pi, mu, sigma, r, b, ll_curr\n",
    "\n",
    "def generalized_skew_normal_pdf(x, mu, sigma, b, r):\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    normalization = b / (2 ** (1 + 1 / b) * gamma(1 / b) * sigma)\n",
    "    denom = sigma * (1 + r * np.sign(x - mu))\n",
    "    denom = np.maximum(denom, 1e-12)\n",
    "    hh = np.abs(x - mu) / denom\n",
    "    pdf = normalization * np.exp(-(hh ** b) / 2.0)\n",
    "    return np.maximum(pdf, 1e-300)\n",
    "\n",
    "def generalized_skew_normal_rvs(mu, sigma, b, r, size):\n",
    "    samples = []\n",
    "    while len(samples) < size:\n",
    "        x_candidate = np.random.uniform(mu - 20 * sigma, mu + 20 * sigma)\n",
    "        u = np.random.uniform(0, 1)\n",
    "        if u < generalized_skew_normal_pdf(x_candidate, mu, sigma, b, r):\n",
    "            samples.append(x_candidate)\n",
    "    return np.array(samples)\n",
    "\n",
    "def ks_test_mixture(data, pi, mu, sigma, b, r, size):\n",
    "    n_components = len(b)\n",
    "    samples = []\n",
    "    for i in range(n_components):\n",
    "        samples_i = generalized_skew_normal_rvs(mu[i], sigma[i], b[i], r[i], size=size)\n",
    "        samples.append(samples_i)\n",
    "\n",
    "    samples_mixed = []\n",
    "    for i in range(n_components):\n",
    "        num_samples = int(pi[i] * size)\n",
    "        samples_mixed.append(samples[i][:num_samples])\n",
    "\n",
    "    samples_mixed1 = np.concatenate(samples_mixed) if len(samples_mixed) > 0 else np.array([])\n",
    "    if len(samples_mixed1) == 0:\n",
    "        return 1.0, 0.0\n",
    "    ks_statistic, p_value = stats.ks_2samp(data, samples_mixed1)\n",
    "    return ks_statistic, p_value\n",
    "\n",
    "def detect_kde_peaks(samples_mixed, height=0.01, grid_n=1000):\n",
    "    samples_mixed2 = np.array(samples_mixed).reshape(-1, 1)\n",
    "    n_samples = len(samples_mixed2)\n",
    "    std_dev = np.std(samples_mixed2)\n",
    "    bandwidth_kde = 1.06 * std_dev * (n_samples ** (-1 / 5)) if std_dev > 0 else 0.1\n",
    "\n",
    "    kde = KernelDensity(bandwidth=bandwidth_kde).fit(samples_mixed2)\n",
    "    x_vals = np.linspace(np.min(samples_mixed2), np.max(samples_mixed2), grid_n)\n",
    "    log_density = kde.score_samples(x_vals.reshape(-1, 1))\n",
    "    density = np.exp(log_density)\n",
    "\n",
    "    peaks, props = find_peaks(density, height=height)\n",
    "    peak_positions = x_vals[peaks]\n",
    "    peak_heights = props.get(\"peak_heights\", density[peaks])\n",
    "    return samples_mixed2, peak_positions, peak_heights\n",
    "\n",
    "def half_range_mode_auto_h(data):\n",
    "    data = np.sort(np.asarray(data))\n",
    "    n = len(data)\n",
    "    if n == 0:\n",
    "        return 0.0, 0.0\n",
    "    m = max(1, n // 2)\n",
    "    data_range = np.max(data) - np.min(data)\n",
    "    h = data_range / m if m > 0 else 0.0\n",
    "\n",
    "    max_count = 0\n",
    "    mode_estimate = float(np.mean(data))\n",
    "    for i in range(n):\n",
    "        j = i\n",
    "        while j < n and data[j] <= data[i] + h:\n",
    "            j += 1\n",
    "        count = j - i\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            mode_estimate = (data[i] + data[j - 1]) / 2\n",
    "    return float(mode_estimate), float(h)\n",
    "\n",
    "def sgn_log_likelihood(theta, x):\n",
    "    mu, sigma, r, b = theta\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    if sigma <= 0 or b <= 0:\n",
    "        return -np.inf\n",
    "\n",
    "    denominator = sigma * (1 + r * np.sign(x - mu))\n",
    "    if np.any(denominator == 0):\n",
    "        return -np.inf\n",
    "    hhh = np.abs(x - mu) / denominator\n",
    "\n",
    "    try:\n",
    "        ll = (\n",
    "            -n * ((1 + 1 / b) * np.log(2) + gamma(1 / b) + np.log(sigma) - np.log(b))\n",
    "            - np.sum(0.5 * (hhh ** b))\n",
    "        )\n",
    "    except Exception:\n",
    "        return -np.inf\n",
    "    return ll\n",
    "\n",
    "def return_initialparams_for_K(samples_mixed, K_target, height=0.01):\n",
    "    samples_mixed2, peak_positions, peak_heights = detect_kde_peaks(samples_mixed, height=height)\n",
    "\n",
    "    if len(peak_positions) == 0 or len(peak_positions) < K_target:\n",
    "        peak_positions = np.array([np.mean(samples_mixed2)])\n",
    "        K_target = 1\n",
    "\n",
    "    order = np.argsort(-peak_heights)\n",
    "    chosen = order[:K_target]\n",
    "    peak_positions = peak_positions[chosen]\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(peak_positions.reshape(-1, 1))\n",
    "    _, labels = nn.kneighbors(samples_mixed2)\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    cluster_sizes = np.bincount(labels, minlength=K_target)\n",
    "    total_points = len(samples_mixed2)\n",
    "\n",
    "    pi_init = cluster_sizes / np.maximum(total_points, 1)\n",
    "    pi_init = np.maximum(pi_init, 1e-12)\n",
    "    pi_init = pi_init / np.sum(pi_init)\n",
    "\n",
    "    mu_init = np.zeros(K_target, dtype=np.float64)\n",
    "    sigma_init = np.zeros(K_target, dtype=np.float64)\n",
    "    r_init = np.zeros(K_target, dtype=np.float64)\n",
    "    b_init = np.zeros(K_target, dtype=np.float64)\n",
    "\n",
    "    for k in range(K_target):\n",
    "        cluster_points = samples_mixed2[labels == k].flatten()\n",
    "        if len(cluster_points) == 0:\n",
    "            mu_init[k] = float(np.mean(samples_mixed2))\n",
    "            sigma_init[k] = float(np.std(samples_mixed2) + 1e-8)\n",
    "            r_init[k] = 0.0\n",
    "            b_init[k] = 2.0\n",
    "            continue\n",
    "\n",
    "        mode, _ = half_range_mode_auto_h(cluster_points)\n",
    "        mu_init[k] = mode\n",
    "\n",
    "        sigma_init[k] = np.sqrt(np.mean((cluster_points - mu_init[k]) ** 2))\n",
    "        sigma_init[k] = max(float(sigma_init[k]), 0.1)\n",
    "\n",
    "        I = (cluster_points <= mu_init[k]).astype(int)\n",
    "        r_init[k] = 1 - 2 * np.mean(I)\n",
    "\n",
    "        b_values = np.linspace(0.1, 100, 2000)  \n",
    "        lls = []\n",
    "        for bb in b_values:\n",
    "            theta = (mu_init[k], sigma_init[k], r_init[k], bb)\n",
    "            lls.append(sgn_log_likelihood(theta, cluster_points))\n",
    "        b_init[k] = float(b_values[int(np.argmax(lls))])\n",
    "\n",
    "    return (pi_init, mu_init, sigma_init, r_init, b_init)\n",
    "\n",
    "def fit_best_by_ks(data, samples_mixed, ks_size=2000, height=0.01,\n",
    "                   em_max_iter=300, em_tol=1e-9):\n",
    "    _, peak_positions, _ = detect_kde_peaks(samples_mixed, height=height)\n",
    "    k_peaks_raw = len(peak_positions)\n",
    "    if k_peaks_raw <= 0:\n",
    "        k_peaks_raw = 1\n",
    "\n",
    "    results = []\n",
    "    best = None\n",
    "\n",
    "    for K in range(k_peaks_raw, 0, -1):\n",
    "        initial_params = return_initialparams_for_K(samples_mixed, K_target=K, height=height)\n",
    "\n",
    "        pi, mu, sigma, r, b, ll = em_algorithm(\n",
    "            data, initial_params, max_iter=em_max_iter, tol=em_tol\n",
    "        )\n",
    "        ks_stat, p_value = ks_test_mixture(data, pi, mu, sigma, b, r, size=ks_size)\n",
    "\n",
    "        rec = {\n",
    "            \"K\": K, \"p_value\": p_value, \"ks_stat\": ks_stat,\n",
    "            \"pi\": pi, \"mu\": mu, \"sigma\": sigma, \"r\": r, \"b\": b, \"loglik\": ll\n",
    "        }\n",
    "        results.append(rec)\n",
    "\n",
    "        if best is None or p_value > best[\"p_value\"]:\n",
    "            best = rec\n",
    "\n",
    "    for rec in results:\n",
    "        print(f\"K={rec['K']}, KS={rec['ks_stat']:.6g}, p-value={rec['p_value']:.6g}\")\n",
    "    print(f\"\\nBest: K={best['K']}, p-value={best['p_value']:.6g}, KS={best['ks_stat']:.6g}\")\n",
    "\n",
    "    return best, results, best[\"pi\"], best[\"mu\"], best[\"sigma\"], best[\"r\"], best[\"b\"]\n",
    "\n",
    "\n",
    "def calc_spearman_network(X, feature_names, n_edges_keep):\n",
    "    n = X.shape[1]\n",
    "    corr_list = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            rho, _ = spearmanr(X[:, i], X[:, j])\n",
    "            if np.isnan(rho):\n",
    "                rho = 0\n",
    "            corr_list.append((feature_names[i], feature_names[j], abs(rho)))\n",
    "    corr_list_sorted = sorted(corr_list, key=lambda x: -x[2])[:n_edges_keep]\n",
    "    G = nx.Graph()\n",
    "    for u, v, w in corr_list_sorted:\n",
    "        G.add_edge(u, v, weight=w)\n",
    "    return G\n",
    "\n",
    "def network_intersection(G1, G2):\n",
    "    H = nx.Graph()\n",
    "    edges1 = set(frozenset(e) for e in G1.edges())\n",
    "    edges2 = set(frozenset(e) for e in G2.edges())\n",
    "    common_edges = edges1.intersection(edges2)\n",
    "    for edge in common_edges:\n",
    "        u, v = tuple(edge)\n",
    "        w2 = G2[u][v]['weight']\n",
    "        H.add_edge(u, v, weight=w2)\n",
    "    for n in G1.nodes():\n",
    "        H.add_node(n)\n",
    "    return H\n",
    "\n",
    "def build_mst(G, feature_names):\n",
    "    components = list(nx.connected_components(G))\n",
    "    largest_comp = max(components, key=len)\n",
    "    G_sub = G.subgraph(largest_comp).copy()\n",
    "    T = nx.maximum_spanning_tree(G_sub)\n",
    "    root = list(G_sub.nodes)[0]\n",
    "    bfs = nx.bfs_tree(T, root)\n",
    "    idx = {g: i for i, g in enumerate(feature_names)}\n",
    "    parents = [-1] * len(feature_names)\n",
    "    for u in bfs.nodes:\n",
    "        if u == root:\n",
    "            continue\n",
    "        try:\n",
    "            p = next(bfs.predecessors(u))\n",
    "            parents[idx[u]] = idx[p]\n",
    "        except StopIteration:\n",
    "            continue\n",
    "    for g in feature_names:\n",
    "        if g not in G_sub.nodes:\n",
    "            parents[idx[g]] = -1\n",
    "    return parents\n",
    "\n",
    "class TAN_GSN_Classifier:\n",
    "    def __init__(self, height=0.01, em_max_iter=300, em_tol=1e-9):\n",
    "        self.class_probs = {}\n",
    "        self.parents = {}\n",
    "        self.class_params = {}\n",
    "        self.regressors = {}\n",
    "        self.height = height\n",
    "        self.em_max_iter = em_max_iter\n",
    "        self.em_tol = em_tol\n",
    "\n",
    "    def fit(self, X, y, feature_names, string_edge_file):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        edges = pd.read_csv(string_edge_file)\n",
    "        string_edges = set(frozenset([r.node1, r.node2]) for r in edges.itertuples())\n",
    "        n_edges_keep = len(string_edges)\n",
    "\n",
    "        for c in self.classes:\n",
    "            self.class_probs[c] = np.mean(y == c)\n",
    "\n",
    "        self.parents = {}\n",
    "        for c in self.classes:\n",
    "            Xc = X[y == c]\n",
    "            spearman_net = calc_spearman_network(Xc, feature_names, n_edges_keep)\n",
    "\n",
    "            string_net = nx.Graph()\n",
    "            for gene in feature_names:\n",
    "                string_net.add_node(gene)\n",
    "            for r in edges.itertuples():\n",
    "                u, v, w = r.node1, r.node2, r.combined_score\n",
    "                if u in feature_names and v in feature_names:\n",
    "                    string_net.add_edge(u, v, weight=w)\n",
    "\n",
    "            intersect_net = network_intersection(spearman_net, string_net)\n",
    "            parents = build_mst(intersect_net, feature_names)\n",
    "            self.parents[c] = parents\n",
    "\n",
    "        self.class_params = {c: [{} for _ in range(n_features)] for c in self.classes}\n",
    "        self.regressors = {c: [None] * n_features for c in self.classes}\n",
    "\n",
    "        for c in self.classes:\n",
    "            Xc = X[y == c]\n",
    "            parent_list = self.parents[c]\n",
    "\n",
    "            for i in range(n_features):\n",
    "                vals = Xc[:, i]\n",
    "                p = parent_list[i]\n",
    "\n",
    "                if p == -1:\n",
    "                    samples_mixed = vals.astype(float)\n",
    "\n",
    "                    best, all_results, pi, mu, sigma, r, b = fit_best_by_ks(\n",
    "                        data=samples_mixed,\n",
    "                        samples_mixed=samples_mixed,\n",
    "                        ks_size=len(samples_mixed),\n",
    "                        height=self.height,\n",
    "                        em_max_iter=self.em_max_iter,\n",
    "                        em_tol=self.em_tol\n",
    "                    )\n",
    "\n",
    "                    self.class_params[c][i][None] = (\"mix\", pi, mu, sigma, r, b)\n",
    "                    self.regressors[c][i] = None\n",
    "\n",
    "                else:\n",
    "                    parent_vals = Xc[:, p].reshape(-1, 1)\n",
    "                    lr = LinearRegression().fit(parent_vals, vals)\n",
    "                    resid = (vals - lr.predict(parent_vals)).astype(float)\n",
    "\n",
    "                    samples_mixed = resid\n",
    "                    best, all_results, pi, mu, sigma, r, b = fit_best_by_ks(\n",
    "                        data=samples_mixed,\n",
    "                        samples_mixed=samples_mixed,\n",
    "                        ks_size=len(samples_mixed),\n",
    "                        height=self.height,\n",
    "                        em_max_iter=self.em_max_iter,\n",
    "                        em_tol=self.em_tol\n",
    "                    )\n",
    "\n",
    "                    self.class_params[c][i][None] = (\"mix\", pi, mu, sigma, r, b)\n",
    "                    self.regressors[c][i] = lr\n",
    "\n",
    "    def _mixture_pdf(self, x, pi, mu, sigma, r, b):\n",
    "        # x: scalar\n",
    "        pdf = 0.0\n",
    "        K = len(pi)\n",
    "        for k in range(K):\n",
    "            pdf += pi[k] * generalized_skew_normal_pdf(x, mu[k], sigma[k], b[k], r[k])\n",
    "        return np.maximum(pdf, 1e-300)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            best_c, best_logp = None, -1e300\n",
    "            for c in self.classes:\n",
    "                lp = np.log(np.maximum(self.class_probs[c], 1e-300))\n",
    "\n",
    "                for i, val in enumerate(x):\n",
    "                    p = self.parents[c][i]\n",
    "                    tag, pi, mu, sigma, r, b = self.class_params[c][i][None]\n",
    "\n",
    "                    if p == -1:\n",
    "                        pdf = self._mixture_pdf(val, pi, mu, sigma, r, b)\n",
    "                    else:\n",
    "                        mu_hat = self.regressors[c][i].predict([[x[p]]])[0]\n",
    "                        pdf = self._mixture_pdf(val - mu_hat, pi, mu, sigma, r, b)\n",
    "\n",
    "                    lp += np.log(np.maximum(pdf, 1e-300))\n",
    "\n",
    "                if lp > best_logp:\n",
    "                    best_logp, best_c = lp, c\n",
    "\n",
    "            preds.append(best_c)\n",
    "        return np.array(preds)\n",
    "\n",
    "\n",
    "train_cancer = pd.read_csv(\n",
    "    r\"Acancer_expression_batch_corrected.csv\",\n",
    "    index_col=0\n",
    ").apply(pd.to_numeric)\n",
    "\n",
    "train_normal = pd.read_csv(\n",
    "    r\"Anormal_expression_batch_corrected.csv\",\n",
    "    index_col=0\n",
    ").apply(pd.to_numeric)\n",
    "\n",
    "test_cancer = pd.read_csv(\n",
    "    r\"Bcancer_expression_batch_corrected.csv\",\n",
    "    index_col=0\n",
    ").apply(pd.to_numeric)\n",
    "\n",
    "test_normal = pd.read_csv(\n",
    "    r\"Bnormal_expression_batch_corrected.csv\",\n",
    "    index_col=0\n",
    ").apply(pd.to_numeric)\n",
    "\n",
    "genes_in_pathway = pd.read_csv(\n",
    "    r\"HSA05226_genes.csv\",\n",
    "    header=None\n",
    ")[0].tolist()\n",
    "\n",
    "common_genes = train_cancer.index.intersection(train_normal.index).intersection(\n",
    "    test_cancer.index).intersection(test_normal.index)\n",
    "\n",
    "features = [g for g in common_genes if g in genes_in_pathway]\n",
    "\n",
    "X_train_df = pd.concat([train_normal, train_cancer], axis=1).T\n",
    "X_train_df = X_train_df[features]\n",
    "X_train = X_train_df.values\n",
    "y_train = np.array([0] * train_normal.shape[1] + [1] * train_cancer.shape[1])\n",
    "\n",
    "X_test_df = pd.concat([test_normal, test_cancer], axis=1).T\n",
    "X_test_df = X_test_df[features]\n",
    "X_test = X_test_df.values\n",
    "y_test = np.array([0] * test_normal.shape[1] + [1] * test_cancer.shape[1])\n",
    "\n",
    "string_edge_file = r\"HSA05226.csv\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = TAN_GSN_Classifier(height=0.01, em_max_iter=300, em_tol=1e-9)\n",
    "clf.fit(X_train_scaled, y_train, features, string_edge_file)\n",
    "\n",
    "N_RUNS = 50\n",
    "TEST_RATIO = 0.8\n",
    "\n",
    "out_dir = r\"result\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "n_test = X_test_scaled.shape[0]\n",
    "n_sample = int(n_test * TEST_RATIO)\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    np.random.seed(run)\n",
    "    idx = np.random.choice(n_test, n_sample, replace=False)\n",
    "\n",
    "    X_test_sub = X_test_scaled[idx]\n",
    "    y_test_sub = y_test[idx]\n",
    "\n",
    "    y_pred = clf.predict(X_test_sub)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_sub, y_pred).ravel()\n",
    "\n",
    "    sen = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spe = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    youden = sen + spe - 1\n",
    "    acc = accuracy_score(y_test_sub, y_pred)\n",
    "    f1 = f1_score(y_test_sub, y_pred)\n",
    "\n",
    "    results.append([run + 1, acc, f1, sen, spe, youden])\n",
    "    print(f\"Run {run+1:02d}: ACC={acc:.4f}, F1={f1:.4f}, Sens={sen:.4f}, Spec={spe:.4f}, Youden={youden:.4f}\")\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Run\", \"ACC\", \"F1\", \"Sens\", \"Spec\", \"Youden\"])\n",
    "out_csv = os.path.join(out_dir, \"tan_gsn_test_results_80perc_random50.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test results saved to:\\n{out_csv}\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
